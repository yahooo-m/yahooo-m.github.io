---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a 4th-year Ph.D candidate at University of Science and Technology of China (USTC), under the supervision of [Dong Liu](https://faculty.ustc.edu.cn/dongeliu/).

I'm generally interested in video self-supervised learning. In particular, on topics:
- video correspondence learning with downstream tasks including video object segmentation, point tracking, etc.
- learning generalizable representations with large-scale unlabled videos
- large-scale generative models for representation and synthesis, e.g., diffusion models, Masked Image Modeling.  




# ğŸ”¥ News
- *2023.07*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted to ICCV 2023
- *2023.02*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted to CVPR 2023
- *2021.06*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted to ICCV 2021 as <font color=red>Oral Presentation</font>
- *2019.09*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted to ACM MM 2019

# ğŸ“ Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2023</div><img src='images/pt1.gif' alt="sym" width="70%"></div></div>
<div class='paper-box-text' markdown="1">

Learning Fine-Grained Features for Pixel-wise Video Correspondences  [[pdf](https://arxiv.org/pdf/2209.07778.pdf)][[code](https://github.com/qianduoduolr/Spa-then-Temp)]

**Rui Li**,  Shenglong Zhou, and Dong Liu

IEEE/CVF International Conference on Computer Vision **(ICCV)** 2023

<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Learn the fine-grained features for pixel-level video correspondences by training on a mixture of labeled synthetic data and unlabeled real data

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/vos1.gif' alt="sym" width="70%"></div></div>
<div class='paper-box-text' markdown="1">

Spatial-then-Temporal Self-Supervised Learning for Video Correspondence  [[pdf](https://arxiv.org/pdf/2209.07778.pdf)][[code](https://github.com/qianduoduolr/Spa-then-Temp)]

**Rui Li**,  Dong Liu

IEEE/CVF Conference on Computer Vision and Pattern Recognition **(CVPR)** 2023

<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Propose a spatial-thentemporal self-supervised learning method to find video correspondence

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021</div><img src='images/framework_iccv2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Motion-Focused Contrastive Learning of Video Representations  [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Motion-Focused_Contrastive_Learning_of_Video_Representations_ICCV_2021_paper.pdf)][[code](https://github.com/YihengZhang-CV/MCL-Motion-Focused-Contrastive-Learning)]

**Rui Li**, Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, and Tao Mei 

IEEE/CVF International Conference on Computer Vision **(ICCV)** 2021  <font color=red>(Oral, top 3%)</font>

<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Leverage motion information to boost self-supervised video representation learning
- Selected as <font color=red>Oral Presentation (top 3%)</font> on ICCV 2021
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2019</div><img src='images/framework_mm2019.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

BERT4SessRec: Content-based video relevance prediction with bidirectional encoder representations from transformer  [[pdf](https://dl.acm.org/doi/pdf/10.1145/3343031.3356051)] [[code](https://github.com/cbvrp-acmmm-2019/cbvrp-acmmm-2019)]

Xusong Chen, Dong Liu, Chenyi Lei, **Rui Li**, Zheng-Jun Zha, Zhiwei Xiong.

ACM Multimedia **(MM)** 2019

<strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Winner of the Content-Based Video Relevance Prediction Challenge
</div>
</div>


# ğŸ– Services
Reviewer of CVPR 2023, CVPR 2022, and TMM.

<!-- # ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# ğŸ’» Internships
- *2020.10 - 2021.08*, JD AI Research, Beijing, China.